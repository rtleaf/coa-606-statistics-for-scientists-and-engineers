<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Fundamentals_of_Statistics.knit</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="syllabus.html">Course Syllabus</a>
</li>
<li>
  <a href="Fundamentals_of_Statistics.html">Fundamentals</a>
</li>
<li>
  <a href="Linear_Models.html">Linear Models</a>
</li>
<li>
  <a href="Maximum_Likelihood.html">Maximum Liklihood</a>
</li>
<li>
  <a href="Multilevel_Models.html">Multilevel Models</a>
</li>
<li>
  <a href="Comp_Intensive_Approaches.html">Resampling Methods</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">




</div>


<div id="introduction-to-statistical-analysis" class="section level1">
<h1><span class="header-section-number">1</span> Introduction to statistical analysis</h1>
<p>Kachigan (1986) defines statistical analysis as the process of data</p>
<ul>
<li><p>Collection,</p></li>
<li><p>Organization, and</p></li>
<li><p>Interpretation.</p></li>
</ul>
<p>In this course we will use the framework above and focus on aspects of collection (experimental design and random sampling), organization (using descriptive and inferential approaches), and interpretation.</p>
<p>One of the primary facets of quantitative analysis is the need to be creative in your approaches - the methods outlined in this class provide the foundation for analysis but the practitioner is encouraged to explore the methods and practices in their discipline to best address the needs for interpretation.</p>
<p>It is my sincere hope that the aspects of collection, organization, and interpretation of data that are presented in this class provide the foundation you will need to establish statistical expertise in your own scientific practice.</p>
<p>One of the features of statistical analysis that I would like to highlight: Every problem, every research approach, will require consideration of the means of collection, processing of data, and statistical analysis.</p>
</div>
<div id="data-organization-and-best-practices" class="section level1">
<h1><span class="header-section-number">2</span> Data organization and best practices</h1>
<p>Broman and Woo (2018) highlight the challenges and best practices for using spreadsheets. The design principles that they outline are useful for your work in MS Excel as well as a variety of specialized statistical programs (i.e. Matlab, R, and many others).</p>
<ul>
<li><p>Spreadsheets continue to to be a primary way for data storage, analysis, and visualization.</p></li>
<li><p>Multipurpose (positive and negative).</p></li>
<li><p>Can be error prone to make large sweeping changes - hard to retrace your steps.</p></li>
<li><p>Organization is critical for reproducible research and archiving. A common scenario in your scientific practice is to have multiple time-sensitive projects occurring simultaneously, organization is critical.</p></li>
</ul>
<div id="consistency-and-conventions" class="section level2">
<h2><span class="header-section-number">2.1</span> Consistency and conventions</h2>
<ul>
<li><p>Use consistent codes for categorical variables.</p></li>
<li><p>Use a consistent fixed code for any missing values (e.g. ‘NA’ or an unfilled cell).</p></li>
<li><p>Use consistent variable names (these are column names).</p></li>
<li><p>Use a consistent data layout in multiple files (same column names). This allows data to be merged in a seamless way. So, each sheet will use similar naming conventions.</p></li>
<li><p>Use a consistent format for all dates (e.g. YMD, DMY).</p></li>
<li><p>Use consistent phrases in your notes. <em>Notes are data.</em> So, treating these as variables that have a binary, nominal, or ordinal value will allow you to treat these them quantitatively.</p></li>
<li><p>Be careful about extra spaces within cells. Again, MS Excel will read these as character codes that must be post-processed for analysis.</p></li>
<li><p>One variable is recorded in each cell (remember, in our scheme, a comment is a variables).</p></li>
<li><p>Strive for a rectangular data layout.</p></li>
<li><p>Avoid font and cell colors as annotation</p></li>
<li><p>Use .csv or some other file back up.</p></li>
</ul>
</div>
<div id="data-managment-adapted-from-malin-pinsky-at-rutgers-university" class="section level2">
<h2><span class="header-section-number">2.2</span> Data Managment (Adapted from Malin Pinsky at Rutgers University)</h2>
<ul>
<li><p>Keep lab notebooks to record what you did, learned, or produced each day. Can be physical notebooks, text files, Evernote, Jupyter notebooks, etc.</p></li>
<li><p>Establish mechanisms that facilitate collaboration and sharing.</p></li>
<li><p>Given the resources available here it is sensible to work directly in the cloud (Dropbox, OneDrive)</p></li>
<li><p>Use descriptive (non-ambiguous) file names</p></li>
<li><p>Keep raw (unprocessed, un-formatted) data in a file and associated directory that is not overwritten. Instead, processing should be performed on this data and saved in an appropriately named file.</p></li>
<li><p>If we ‘clean’ the data, we often use a folder called something like “data-raw” and a folder called “data-clean” to differentiate data in its original form from data that has been manipulated. Have “master” or “original” and “tidy” versions of files and name them appropriately.</p></li>
<li><p>Store raw data with metadata describing the contents of the file (i.e. what do the columns mean, how was the data collected, etc.)</p></li>
<li><p>If using data downloaded from another data source, we often have a folder called “data_dl” for ‘downloaded data’. Include the data source in a README file for reproducibility (this is like meta data and provides useful description).</p></li>
</ul>
</div>
</div>
<div id="types-of-variables" class="section level1">
<h1><span class="header-section-number">3</span> Types of Variables</h1>
<p>Properties and characteristics of an object that can assume two or more different values are called <em>variables</em>. These are the values that comprise our data.</p>
<p>We need to understand the structure of variables - different types of measurements will require different methods of analysis.</p>
<p>The next step after data collection is to organize the data into a meaningful form so that patterns and contrasts can be seen easily.</p>
<p>One of the common methods for organizing univariate data is to construct frequency distributions either in a table or as a figure. The frequency distribution is an organized representation of the number of observations in each category on the scale of measurement. These allow researchers to have a glance at the entire data conveniently.</p>
<p>It shows whether the observations are high or low and also whether they are concentrated in one area or spread out across the entire scale.</p>
<div id="categorical-variables" class="section level2">
<h2><span class="header-section-number">3.1</span> Categorical Variables</h2>
<p>Entities are divided into distinct categories.</p>
<div id="binary-variable" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Binary variable</h3>
<p>There are only two categories, e.g.</p>
<ul>
<li><p>dead or alive,</p></li>
<li><p>present or absent,</p></li>
<li><p>positive or negative (e.g. for a disease),</p></li>
<li><p>the value of some quantity of interest is zero or positive,</p></li>
<li><p>or the value of some quantity of interest exceeds some threshold value.</p></li>
</ul>
</div>
<div id="nominal-variable" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Nominal variable</h3>
<p>There are more than two categories, e.g.</p>
<ul>
<li><p>Whether the subject is an omnivore, vegetarian, vegan, or carnivore.</p></li>
<li><p>The subject’s taxonomic group,</p></li>
</ul>
</div>
<div id="ordinal-variable" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Ordinal variable</h3>
<p>Similar to a nominal variable but the categories are ordered.</p>
<ul>
<li><p>Whether people got a fail, a pass, a merit or a distinction in their exam.</p></li>
<li><p>Intensity of infection (e.g. none, mild, moderate, severe)</p></li>
</ul>
</div>
<div id="frequency-distribution-of-a-categorical-variable" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Frequency Distribution of a Categorical Variable</h3>
<ul>
<li>A tally of how frequently occurring a value is among categories.</li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
</div>
<div id="continuous-variables" class="section level2">
<h2><span class="header-section-number">3.2</span> Continuous Variables</h2>
<p>Equal intervals on the variable represent equal differences in the property being measured, e.g.</p>
<ul>
<li><p>the difference between 6 and 8 is equivalent to the difference between 13 and 15,</p></li>
<li><p>Density or frequency of organisms in a transect or at a sampling station,</p></li>
<li><p>Body Mass Index or some measure of condition of an organism.</p></li>
</ul>
<div id="frequency-distribution-of-a-continuous-variable" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Frequency Distribution of a Continuous Variable</h3>
<p>A question often asked: What interval to choose? Your knowledge of the domain will guide this. The width of the class can be determined by dividing the range of observations by the number of classes.</p>
<p>The following are some guidelines regarding class widths:</p>
<p>1.) It is advisable to have equal class widths. Unequal class widths should be used only when large gaps exist in data.</p>
<p>2.) The class intervals should be mutually exclusive and nonoverlapping.</p>
<p>3.) Open-ended classes at the lower and upper side (e.g., &lt;10, &gt;100) should be avoided.</p>
<center>
<p>Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data</p>
<table>
<thead>
<tr class="header">
<th>Phosphorous concentration</th>
<th>Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8.1 to 8.2</td>
<td>2</td>
</tr>
<tr class="even">
<td>8.2 to 8.3</td>
<td>6</td>
</tr>
<tr class="odd">
<td>8.3 to 8.4</td>
<td>8</td>
</tr>
<tr class="even">
<td>8.4 to 8.5</td>
<td>11</td>
</tr>
<tr class="odd">
<td>8.5 to 8.6</td>
<td>17</td>
</tr>
<tr class="even">
<td>8.6 to 8.7</td>
<td>17</td>
</tr>
<tr class="odd">
<td>8.7 to 8.8</td>
<td>24</td>
</tr>
<tr class="even">
<td>8.8 to 8.9</td>
<td>18</td>
</tr>
<tr class="odd">
<td>8.9 to 9.0</td>
<td>13</td>
</tr>
<tr class="even">
<td>9.0 to 9.1</td>
<td>10</td>
</tr>
<tr class="odd">
<td>9.1 to 9.2</td>
<td>4</td>
</tr>
</tbody>
</table>
Total frequency = 130 = n
</center>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
</div>
<div id="cumulative-distribution-of-a-continuous-variable" class="section level2">
<h2><span class="header-section-number">3.3</span> Cumulative Distribution of a Continuous Variable</h2>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>Cummulative Frequency</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Phosphorous concentration</td>
<td>Frequency</td>
<td>Starting with Low Values</td>
<td>Starting with High Values</td>
</tr>
<tr class="even">
<td>8.15 to 8.25</td>
<td>2</td>
<td>2</td>
<td>130</td>
</tr>
<tr class="odd">
<td>8.25 to 8.35</td>
<td>6</td>
<td>8</td>
<td>128</td>
</tr>
<tr class="even">
<td>8.35 to 8.45</td>
<td>8</td>
<td>16</td>
<td>122</td>
</tr>
<tr class="odd">
<td>8.45 to 8.55</td>
<td>11</td>
<td>27</td>
<td>114</td>
</tr>
<tr class="even">
<td>8.55 to 8.65</td>
<td>17</td>
<td>44</td>
<td>130</td>
</tr>
<tr class="odd">
<td>8.65 to 8.75</td>
<td>17</td>
<td>61</td>
<td>86</td>
</tr>
<tr class="even">
<td>8.75 to 8.85</td>
<td>24</td>
<td>85</td>
<td>69</td>
</tr>
<tr class="odd">
<td>8.85 to 8.95</td>
<td>18</td>
<td>103</td>
<td>45</td>
</tr>
<tr class="even">
<td>8.95 to 9.05</td>
<td>13</td>
<td>116</td>
<td>27</td>
</tr>
<tr class="odd">
<td>9.05 to 9.15</td>
<td>10</td>
<td>126</td>
<td>14</td>
</tr>
<tr class="even">
<td>9.15 to 9.25</td>
<td>4</td>
<td>130</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Total frequency = 130 = n</p>
<div id="skew-and-kurtosis-of-a-continuous-variable" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Skew and Kurtosis of a Continuous Variable</h3>
<ul>
<li>A left-skewed distribution has a long left tail.</li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<ul>
<li>A right-skewed distribution has a long right tail. <img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-5-1.png" width="672" /></li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="frequency-polygon-of-a-continuous-variable" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Frequency Polygon of a Continuous Variable</h3>
<p>A frequency polygon is constructed by connecting all midpoints of the top of the bars in a histogram by a straight line without displaying the bars.</p>
<p>A frequency polygon aids in the easy comparison of two frequency distributions.</p>
<p>When the total frequency is large and the class intervals are narrow, the frequency polygon becomes a smooth curve known as the frequency curve.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="box-and-whisker-plot-of-a-continuous-variable" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Box and Whisker Plot of a Continuous Variable</h3>
<p>This graph, first described by Tukey in 1977, can also be used to illustrate the distribution of data. There is a vertical or horizontal rectangle (box), the ends of which correspond to the upper and lower quartiles (75th and 25th percentile, respectively).</p>
<p>Hence the middle 50% of observations are represented by the box.</p>
<p>The length of the box indicates the variability of the data. The line inside the box denotes the median (sometimes marked as a plus sign).</p>
<p>The position of the median indicates whether the data are skewed or not. If the median is closer to the upper quartile, then they are negatively skewed and if it is near the lower quartile, then positively skewed.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="violin-plots" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Violin Plots</h3>
<p>Violin plots are an alternative to box plots that solves the issues regarding displaying the underlying distribution of the observations, as these plots show a kernel density estimate of the data.</p>
<p>same summary statistics as box plots:</p>
<ul>
<li><p>the white dot represents the median.</p></li>
<li><p>the thick gray bar in the center represents the interquartile range.</p></li>
<li><p>the thin gray line represents the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the interquartile range.</p></li>
</ul>
<p>On each side of the gray line is a kernel density estimation to show the distribution shape of the data. Wider sections of the violin plot represent a higher probability that members of the population will take on the given value; the skinnier sections represent a lower probability.</p>
<pre><code>## Warning: package &#39;vioplot&#39; was built under R version 4.0.5</code></pre>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
</div>
<div id="descriptive-statistics-of-continuous-frequency-distributions" class="section level2">
<h2><span class="header-section-number">3.4</span> Descriptive Statistics of Continuous Frequency Distributions</h2>
<div id="range" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Range</h3>
<p>The smallest score subtracted from the largest score in the observation, e.g. </p>
<ul>
<li>Number of friends of 11 Facebook users.</li>
<li>22, 40, 53, 57, 93, 98, 103, 108, 116, 121, 252</li>
<li>Range = 252 - 22 = 230</li>
</ul>
</div>
<div id="the-interquartile-range" class="section level3">
<h3><span class="header-section-number">3.4.2</span> The Interquartile Range</h3>
<p>Identify the values that split the sorted data into four equal parts.</p>
<ul>
<li><p><em>First</em> or <em>lower quartile</em> (the range values of the first 25% of values in ordered sequence)</p></li>
<li><p><em>Second</em> quartile (the range values of the first 25 to 50% of values in ordered sequence)</p></li>
<li><p><em>Third</em> quartile (the range values of the first 50 to 75% of values in ordered sequence)</p></li>
<li><p><em>Fourth</em> quartile (the range values of the first 75 to 100% of values in ordered sequence)</p></li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="measures-of-central-tendency" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Measures of central tendency</h3>
<ul>
<li><p>The mode is the most frequently occurring value in the population or sample.</p></li>
<li><p>The median is the 50th percentile in the ordered data.</p></li>
<li><p>The mean (much more about this below).</p></li>
</ul>
</div>
</div>
</div>
<div id="the-normal-distribution" class="section level1">
<h1><span class="header-section-number">4</span> The Normal Distribution</h1>
<ul>
<li>The normal distribution is probably the most common distribution in all of probability and statistics.</li>
<li>One of the main reasons it crops up so much is due to the Central Limit Theorem (we will explore this).</li>
</ul>
<div id="the-normal-probability-density-function" class="section level2">
<h2><span class="header-section-number">4.1</span> The Normal Probability Density Function</h2>
<p>The probability density function for the normal distribution is defined as:</p>
<p><span class="math inline">\(y_i = \frac{1}{\sigma\sqrt2\pi}e^{-(X_i-\mu)^2/2\sigma^2}\)</span></p>
<p>We can think of the model in this way (mathematical approach):</p>
<p><span class="math inline">\(f(X) = \frac{1}{\sigma\sqrt2\pi}e^{-(X_i-\mu)^2/2\sigma^2}\)</span></p>
<p>Where the parameters (the symbols ) represent the mean, <span class="math inline">\(\mu\)</span> (the point on the x-axis where the center of the distribution is) and the standard deviation, <span class="math inline">\(\sigma\)</span> (how spread out the distribution is) of the population.</p>
<ul>
<li>What are some of the general characteristics of this model? <em>Can you describe its shape?</em></li>
<li>What are the parameters of the model? <em>These are the quantities we will estimate in the fitting process.</em></li>
<li>What are the variables used in the model? <em>These are the observations.</em></li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Below, two distributions are plotted from the ‘Standard Normal Distribution’, in this formulation:</p>
<p><span class="math inline">\({\sigma=1}\)</span> and <span class="math inline">\({\mu=0}\)</span>.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The normal distribution is an example of a continuous univariate probability distribution with infinite support. By infinite support, we mean that we can calculate values of the probability density function for all outcomes between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(+\infty\)</span>.</p>
<p>The output of a probability density function is <em>not a probability value.</em></p>
<p>To get the probability from a probability density function we need to find the area under the curve. So from our example distribution with mean = 0 and standard deviation = 1, we can find the probability that the outcome is between 0 and 1 by finding the area shown in the image below.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<center>
<span class="math inline">\(\int_0^1f(x;\mu,\sigma)dx = P(0 &lt; X &lt; 1)\)</span>
</center>
<p>We can read this as “the integral of the probability density function between 0 and 1 (on the left-hand side) is equal to the probability that the outcome of the random variable is between zero and 1 (on the right-hand side)”.</p>
<p>We can cover all possible values if we set our range from ‘minus infinity’ all the way to ‘positive infinity’. Therefore the following has to be true for the function to be a probability density function:</p>
<center>
<span class="math inline">\(\int_{-\infty}^{\infty}f(x)dx = 1\)</span>.
</center>
<p>One last thing here: The probability of the random variable being equal to a specific outcome is 0, because the integral over x values of x to x is equal to zero.</p>
</div>
<div id="z-scores" class="section level2">
<h2><span class="header-section-number">4.2</span> <em>Z</em>-scores</h2>
<p><em>Z</em>-scores are a way to center and scale the observations.</p>
<p><span class="math inline">\(Z_i = \frac{X_i-\bar{X}}{sd}\)</span></p>
<ul>
<li>Standardizing a score with respect to the other scores in the group.</li>
<li>Expresses a score in terms of how many standard deviations it is away from the mean.</li>
<li>Converts a distribution to a z-score distribution.</li>
<li>Z-scores have mean = 0 and standard deviation = 1.</li>
</ul>
<pre><code>##     mean    sd
## 1 -0.217 1.004</code></pre>
<table>
<colgroup>
<col width="4%" />
<col width="33%" />
<col width="44%" />
<col width="8%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="right">Centered Obs (difference between the value and the mean)</th>
<th align="right">Scaled Centered Obs (Centered value divided by standard deviation, Z-score)</th>
<th align="right">Prob (obs &lt; Z)</th>
<th align="right">1-Prob (Obs &gt; Z)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.599</td>
<td align="right">0.816</td>
<td align="right">0.813</td>
<td align="right">0.792</td>
<td align="right">0.208</td>
</tr>
<tr class="even">
<td align="right">-0.926</td>
<td align="right">-0.709</td>
<td align="right">-0.706</td>
<td align="right">0.240</td>
<td align="right">0.760</td>
</tr>
<tr class="odd">
<td align="right">-2.051</td>
<td align="right">-1.834</td>
<td align="right">-1.827</td>
<td align="right">0.034</td>
<td align="right">0.966</td>
</tr>
<tr class="even">
<td align="right">0.261</td>
<td align="right">0.478</td>
<td align="right">0.476</td>
<td align="right">0.683</td>
<td align="right">0.317</td>
</tr>
<tr class="odd">
<td align="right">-0.715</td>
<td align="right">-0.498</td>
<td align="right">-0.496</td>
<td align="right">0.310</td>
<td align="right">0.690</td>
</tr>
<tr class="even">
<td align="right">0.484</td>
<td align="right">0.700</td>
<td align="right">0.698</td>
<td align="right">0.757</td>
<td align="right">0.243</td>
</tr>
<tr class="odd">
<td align="right">0.425</td>
<td align="right">0.642</td>
<td align="right">0.640</td>
<td align="right">0.739</td>
<td align="right">0.261</td>
</tr>
<tr class="even">
<td align="right">1.352</td>
<td align="right">1.569</td>
<td align="right">1.563</td>
<td align="right">0.941</td>
<td align="right">0.059</td>
</tr>
<tr class="odd">
<td align="right">-0.896</td>
<td align="right">-0.679</td>
<td align="right">-0.676</td>
<td align="right">0.249</td>
<td align="right">0.751</td>
</tr>
<tr class="even">
<td align="right">-0.702</td>
<td align="right">-0.485</td>
<td align="right">-0.483</td>
<td align="right">0.314</td>
<td align="right">0.686</td>
</tr>
</tbody>
</table>
<div id="properties-of-z-scores---quantiles" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Properties of <em>Z</em>-scores - Quantiles</h3>
<ul>
<li><p>1.96 definesthe top 2.5% of the distribution.</p></li>
<li><p>-1.96 defines the bottom 2.5% of the distribution.</p></li>
<li><p>As such, 95% of z-scores lie between -1.96 and 1.96.</p></li>
<li><p>99% of z-scores lie between -2.58 and 2.58.</p></li>
<li><p>99.9% of them lie between -3.29 and 3.29.</p></li>
<li><p>Let’s look at a <a href="http://www.z-table.com/">Z Table</a></p></li>
</ul>
</div>
<div id="areas-under-the-normal-curve-for-different-quantile-values" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Areas under the Normal Curve for different quantile values</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="model-fitting" class="section level1">
<h1><span class="header-section-number">5</span> Model Fitting</h1>
<p>The normal distribution is our first statistical model.</p>
<p>The mean is the hypothetical value of the most common outcome. However, it is not a perfect representation of the characteristics of the data. The Standard Deviation is the measure of dispersion (precision).</p>
<p>So, we will be fitting observations to understand how well the mean represents reality.</p>
<div id="the-objective-function" class="section level2">
<h2><span class="header-section-number">5.1</span> The Objective Function</h2>
<p>The residuals for a particular model:</p>
<p><span class="math inline">\(outcome = model + error\)</span></p>
<p><span class="math inline">\(error_i = observation_i - expected_i\)</span></p>
<p>To ‘thread the model through the middle of the noise’, we want the magnitudes of all residuals to be small. A reasonable way (not the only way) to achieve this is to define our objective function to minimize error.</p>
</div>
<div id="quantifying-error" class="section level2">
<h2><span class="header-section-number">5.2</span> Quantifying Error</h2>
<p>A deviation is the difference between the mean (<em>expected</em>) and the <em>observed</em> data (the outcome of the sample). The deviation of <em>observed</em> and <em>expected</em> value is called: the residual, error, or residual error</p>
<p>In the context of the normal distribution, we can calculate the deviations by calculating the difference between the observations and predictions.</p>
<p>When the normal fitting models:</p>
<p><span class="math inline">\(deviation = X_i - \bar{X}\)</span></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Should we use the Total Error as an estimate of uncertainty?</p>
<p>We could sum <span class="math inline">\(i^{th}\)</span> error terms from 1 to n.</p>
<table>
<thead>
<tr class="header">
<th align="center"><em>i</em></th>
<th align="center">Score</th>
<th align="center">Score - mean(Score)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">-0.28</td>
<td align="center">-0.795</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">-0.14</td>
<td align="center">-0.655</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">1.04</td>
<td align="center">0.525</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">0.65</td>
<td align="center">0.135</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">0.26</td>
<td align="center">-0.255</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">1.56</td>
<td align="center">1.045</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(\Sigma(X_i - \bar{X}) = 0\)</span></p>
</div>
<div id="sum-of-squared-errors" class="section level2">
<h2><span class="header-section-number">5.3</span> Sum of Squared Errors</h2>
<p>We could add the deviations to find out the total error, but the deviations ‘cancel out’ (some are positive and others negative)</p>
<p>Therefore, we square each deviation. If we add these squared deviations we get the sum of squared errors (SS).</p>
<table>
<thead>
<tr class="header">
<th align="center"><em>i</em></th>
<th align="center">Score</th>
<th align="center">Deviation</th>
<th align="center">Squared Deviation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">-0.28</td>
<td align="center">-0.795</td>
<td align="center">0.632</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">-0.14</td>
<td align="center">-0.655</td>
<td align="center">0.429</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">1.04</td>
<td align="center">0.525</td>
<td align="center">0.276</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0.65</td>
<td align="center">0.135</td>
<td align="center">0.018</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.26</td>
<td align="center">-0.255</td>
<td align="center">0.065</td>
</tr>
<tr class="even">
<td align="center">5</td>
<td align="center">1.56</td>
<td align="center">1.045</td>
<td align="center">1.092</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(SS = \Sigma(X_i - \bar{X})^2 = 2.512\)</span></p>
<p>The sum of squares is a good measure of overall variability, but is dependent on the number of scores.</p>
</div>
<div id="lets-fit-a-model-to-some-data-using-the-sum-of-squares-criteria" class="section level2">
<h2><span class="header-section-number">5.4</span> Let’s fit a model to some data using the sum of squares criteria:</h2>
<p>Let’s look at data from 10 tree heights. We want to estimate the mean value. To do this we will plot the resulting <span class="math inline">\(SS\)</span> (SS.est) resulting from changing the value of <span class="math inline">\(\bar{X}\)</span>.</p>
<p>The code and figure is a simulation that displays what happens to the <em>SS</em> as a result of trying to fit the normal model with different values of <span class="math inline">\(\bar{X}\)</span>.</p>
<p>Look at the curve. You will notice that the best estimate of <span class="math inline">\(\bar{X}\)</span> is found when <em>SS</em> is minimized.</p>
<pre class="r"><code>tree.ht &lt;- c(2,4,9,3,5,1,2,12,8,10)
candidate.mean &lt;- seq(min(tree.ht), max(tree.ht), length.out = 300)
SS.est &lt;- c()
for (j in 1:length(candidate.mean))
  
  SS.est[j] &lt;- sum((tree.ht - candidate.mean[j])^2)

plot(candidate.mean, SS.est, type = &#39;n&#39;)
lines(candidate.mean, SS.est, lwd = 2 )
legend(legend = paste(&quot;Mean of tree height: &quot;,round(mean(tree.ht),2)),&#39;topleft&#39;,bty = &#39;n&#39;)
abline(v = mean(tree.ht), lwd = 2)</code></pre>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="variance" class="section level2">
<h2><span class="header-section-number">5.5</span> Variance</h2>
<p>In statistics, when we talk about population, we mean the entire universe of possible values of a stochastic (random) variable. Population variance:</p>
<p><span class="math inline">\(\sigma^2 = \frac{SS}{N} = \frac{\Sigma^n_{i=1}(X_i-\mu)^2}{N}\)</span></p>
<p>Most of the time, we don’t sample the entire population because it is too complex or simply not feasible. Think, for instance, at a problem when you want to analyze the heights of the oak trees in a forest. You can, of course, measure every single tree of the forest and so have collected statistics about the entire forest, but this could be very expensive and would take a very long time. So, we don’t generally know <span class="math inline">\(\mu\)</span>.</p>
<p>We calculate the average variability (average variability of each sample).</p>
<p>So, obtain a sample of, let’s say, 20 trees and try to relate sample statistics and population statistics.</p>
<p>Sample variance: <span class="math inline">\(s^2 = \frac{SS}{n-1} = \frac{\Sigma(X_i-\bar{X})^2}{n-1}\)</span></p>
<p>Why n-1 instead of N?</p>
<p>When we use N instead of n-1, we have an error called statistical bias, which means that the sample variance (the estimator) is systematically different from the true population parameter (in this case, the variance).</p>
<p>Let’s see an example:</p>
<p>Imagine a forest of 10,000 oak trees: This is the entire population. We want to estimate the distribution of heights.</p>
<p>Suppose we don’t know that the heights are normally distributed with an average of 10 m and a standard deviation (square root of variance) of 2 m. These are the statistical parameters of the entire population. We try to estimate these values through a sample of 20 random oak trees. We repeat the experiment 100 times.</p>
<p>Given that 20 samples are a tiny subset of 10,000 items of population, every time we sample the population, we get a different estimate of the sample variance and the population variance. On average (after taking many, many samples), we get a reasonable estimate of the standard deviation of the population.</p>
</div>
<div id="standard-deviation" class="section level2">
<h2><span class="header-section-number">5.6</span> Standard Deviation</h2>
<p>The variance has one problem: it is measured in units<sup>2</sup> (The original units, like the numbers are squared.).</p>
<p>This isn’t a very meaningful metric so we take the square root value.</p>
<p>This is the sample standard deviation (<em>sd</em> or <em>s</em>):</p>
<p><span class="math inline">\(s = \sqrt\frac{\Sigma^n_{i=1}(X_i-\bar{X})^2}{n-1}\)</span></p>
</div>
<div id="summary-of-variance-estimates" class="section level2">
<h2><span class="header-section-number">5.7</span> Summary of Variance Estimates</h2>
<p>The sum of squares, variance, and standard deviation represent the same thing:</p>
<ul>
<li>The fit of the mean to the data, how well the mean represents the observed data</li>
<li>The variability in the data when modeled using the mean</li>
</ul>
</div>
<div id="central-limit-theorem-clt" class="section level2">
<h2><span class="header-section-number">5.8</span> Central Limit Theorem (CLT)</h2>
<p>The central limit theorem (CLT) states that the distribution of sample means approximates a normal distribution as the sample size becomes larger regardless of the population distribution shape.</p>
<p>The sample theory is the study of relationships existing between a population and samples drawn from population.</p>
<p>Consider all possible samples of size <em>n</em> that can be drawn from the population. For each sample, we can compute statistic like mean or a standard deviation, etc that will vary from sample to sample. This way we obtain a distribution called as the sampling distribution of a statistic. If the statisic is sample mean , then the distribution is called the sampling distribution of mean.</p>
<p>If we take many sets of samples from a population, and calculated the mean of these, we could plot the frequency distribution of the sample means.</p>
<p>This distribution is the ‘sampling distribution’ It:</p>
<p>1.) The sampling distribution from a normal distribution is normally-distributed.</p>
<p>2.) As sample size increases (to infinity), the sampling distribution, from any distribution, will approach a normal distribution.</p>
<p>3.) The expected value (the mean) of the sample distribution will be the mean of the population distribution.</p>
</div>
<div id="example-of-the-clt-1-of-2" class="section level2">
<h2><span class="header-section-number">5.9</span> Example of the CLT (1 of 2)</h2>
<p>A fair die can be modelled with a discrete random variable with outcome 1 through 6, each with the equal probability of 1/6.</p>
<pre class="r"><code>DieOutcome &lt;- sample(1:6,10000, replace= TRUE)
hist(DieOutcome, col =&quot;light blue&quot;)
abline(v=3.5, col = &quot;red&quot;,lty=1)</code></pre>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>We will take samples of size 10 , from the above 10000 observation of outcome of die roll, take the arithmetic mean and try to plot the mean of sample. we will do this procedure k times (in this case k= 10000 )</p>
<pre class="r"><code>x10 &lt;- c()
k =10000
for ( i in 1:k) {
  x10[i] = mean(sample(1:6,10, replace = TRUE))}
hist(x10, col =&quot;pink&quot;, main=&quot;Sample size =10&quot;,xlab =&quot;Outcome of die roll&quot;)
abline(v = mean(x10), col = &quot;Red&quot;)
abline(v = 3.5, col = &quot;blue&quot;)</code></pre>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>By theory , we know as the sample increases, we get better bell shaped curve. As the n apporaches infinity , we get a normal distribution. Lets do this by increasing the sample size to 30, 100 and 1000.</p>
<pre class="r"><code>x30 &lt;- c()
x100 &lt;- c()
x1000 &lt;- c()
k =10000
for ( i in 1:k){
  x30[i] = mean(sample(1:6,30, replace = TRUE))
  x100[i] = mean(sample(1:6,100, replace = TRUE))
  x1000[i] = mean(sample(1:6,1000, replace = TRUE))
}
par(mfrow=c(1,3))
hist(x30, col =&quot;green&quot;,main=&quot;n=30&quot;,xlab =&quot;die roll&quot;)
abline(v = mean(x30), col = &quot;blue&quot;)

hist(x100, col =&quot;light blue&quot;, main=&quot;n=100&quot;,xlab =&quot;die roll&quot;)
abline(v = mean(x100), col = &quot;red&quot;)

hist(x1000, col =&quot;orange&quot;,main=&quot;n=1000&quot;,xlab =&quot;die roll&quot;)
abline(v = mean(x1000), col = &quot;red&quot;)</code></pre>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<div id="example-of-the-clt-1-of-2-1" class="section level3">
<h3><span class="header-section-number">5.9.1</span> Example of the CLT (1 of 2)</h3>
<p>Flipping a fair coin many times the probability of getting a given number of heads in a series of flips should follow a normal curve, with mean equal to half the total number of flips in each series. Here 1 represent heads and 0 tails.</p>
<pre class="r"><code>x &lt;- c()
k =10000  
for ( i in 1:k) {  
  x[i] = mean(sample(0:1,100, replace = TRUE))}  
hist(x, col =&quot;light green&quot;, main=&quot;Sample size = 100&quot;,xlab =&quot;flipping coin &quot;)  
abline(v = mean(x), col = &quot;red&quot;)</code></pre>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="sampling-and-parameter-estimation" class="section level1">
<h1><span class="header-section-number">6</span> Sampling and Parameter Estimation</h1>
<div id="simple-random-samples" class="section level2">
<h2><span class="header-section-number">6.1</span> Simple Random Samples</h2>
<ul>
<li><p>Every member of the population has an equal probability of being included in the sample.</p></li>
<li><p>Consider Exit polling vs. Twitter polls to estimate proportion voting for a candidate. Why does one provide a better estimate than another.</p></li>
<li><p>BTW, this is the primary issue with current political polling - the quest to get a representative sample (<a href="https://fivethirtyeight.com/features/is-the-polling-industry-in-stasis-or-in-crisis/" class="uri">https://fivethirtyeight.com/features/is-the-polling-industry-in-stasis-or-in-crisis/</a>)</p></li>
</ul>
<p>Some language of sampling that is useful to review (or introduce):</p>
<ul>
<li>a <em>population</em> is the entire collection of people or things you are interested in (note a <em>census</em> is a measurement of all the units in the population);</li>
<li>a <em>sampling frame</em> is the specific data from which the sample is drawn, e.g., a telephone book;</li>
<li>a <em>unit of analysis</em> is the type of object of interest, e.g., arsons, fire departments, firefighters;</li>
<li>a <em>sample</em> is a subset of some of the units in the population;</li>
<li>a <em>statistic</em> is a number that results from measuring all the units in the sample;</li>
</ul>
<p>Statistics derived from samples are used to estimate population parameters.</p>
<p>For example, to find out the average age of all motor vehicles in the state in 1997:</p>
<ul>
<li><em>Population</em> = all motor vehicles in the state in 1997</li>
<li><em>Sampling frame</em> = all motor vehicles registered with the DMV on July 1, 1997</li>
<li><em>Unit of analysis</em> = motor vehicle</li>
<li><em>Sample</em> = 300 motor vehicles</li>
<li><em>Statistic</em> = the average age of the 300 motor vehicles in the sample</li>
</ul>
<p>Simple random sample (SRS): <em>Each unit in the population is identified, and each unit has an equal chance of being in the sample.</em> The selection of each unit is independent of the selection of every other unit. Selection of one unit does not affect the chances of any other unit (<em>Independence</em>).</p>
<p>For example, to select a sample of 25 people who live in your college dorm, make a list of all the 250 people who live in the dorm.</p>
<ul>
<li><p>Assign each person a unique number, between 1 and 250.</p></li>
<li><p>Then refer to a table of random numbers.</p></li>
<li><p>Starting at any point in the table, read across or down and note every number that falls between 1 and 250.</p></li>
<li><p>Use the numbers you have found to pull the names from the list that correspond to the 25 numbers you found.</p></li>
<li><p>These 25 people are your sample. This is called the table of random numbers method.</p></li>
</ul>
<p>We (I) spend considerable effort to discuss SRS because it is the way to obtain <em>statistically valid data</em>.</p>
<ul>
<li><p>Only statistically valid data that can be used for analysis - the first assumption of parametric statistics.</p></li>
<li><p>We will generally make an inspection of the data and/or the residuals to ensure that they are normally distributed (The second assumption)</p></li>
<li><p>If we are examining and doing statistical tests among or between groups then the variances of each group must be <em>equal</em>. The third assumption.</p></li>
<li><p>Ensuring that data are collected in a random fashion allows statistics to be calculated. Non-random data collection disallows this.</p></li>
<li><p>Independence is also a critical aspect of sampling - the sampling of one element will not impact or predict the value of another element.</p></li>
</ul>
</div>
<div id="hurlburt-1984" class="section level2">
<h2><span class="header-section-number">6.2</span> Hurlburt (1984)</h2>
<p>Now that we know what constitutes a valid sample, lets look at some ways (and some ways not) to collect samples.</p>
<p>Hurlburt’s paper has been instrumental in promoting an understanding of <em>pseudoreplication</em> and the term has become widely used. Ecologists have become more aware of the need for close concordance of design, analysis, and interpretation of experiments.</p>
<p><em>“No one would now dream of testing the response to a treatment by comparing two plots, one treated and the other control.” Fisher and Wishart (1930).</em></p>
<p>Hurlburt evaluated the frequency of <em>pseudoreplication</em> as a fatal flaw of experiments</p>
<ul>
<li><p>Hurlbert(1984) Ecological Monographs 54:187‐211</p></li>
<li><p>Evaluated 176 studies from 1960 to 1984.</p></li>
<li><p>Found 27% overall or 48% of those making statistical inferences used “pseudoreplication”.</p></li>
<li><p>The term pseudoreplication was coined by Hurlbert to refer to "the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent.“</p></li>
<li><p>The context of his paper was ecological field experiments, but pseudoreplication can occur in other contexts as well.</p></li>
</ul>
<div id="types-of-experiments" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Types of Experiments</h3>
<p>Manipulative Experiments have the features</p>
<ul>
<li>Control</li>
<li>Randomization</li>
<li>Replication</li>
</ul>
<p>What is the other class of experiments that Hurlburt discusses?</p>
</div>
<div id="sample-allocation" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Sample allocation</h3>
<ul>
<li><p>Statisticians emphasize importance of randomization</p></li>
<li><p>Hurlbert’s concern is with randomization methods not always giving good interspersion of treatments (spatial and temporal)</p></li>
</ul>
</div>
<div id="what-is-pseudoreplication" class="section level3">
<h3><span class="header-section-number">6.2.3</span> What is Pseudoreplication?</h3>
<ul>
<li><p>Replication refers to having more than one experimental (or observational) unit with the same treatment.</p></li>
<li><p>Each unit with the same treatment is called a replicate.</p></li>
<li><p>Most models for statistical inference require true replication.</p></li>
<li><p>True replicates are often confused with repeated measures or with pseudoreplicates.</p></li>
</ul>
</div>
<div id="examples-of-pseudoreplication" class="section level3">
<h3><span class="header-section-number">6.2.4</span> Examples of Pseudoreplication</h3>
<ol style="list-style-type: decimal">
<li><p>Suppose a blood-pressure lowering drug is administered to a patient, then the patient’s blood pressure is measured twice. This is a repeated measure, not a replication. It can give information about the uncertainty in the measurement process, but not about the variability in the effect of the drug. On the other hand, if the drug were administered to two patients, and each patient’s blood pressure was measured once, we can say the treatment has been replicated, and the replication may give some information about the variability in the effect of the drug.</p></li>
<li><p>A researcher is studying the effect on plant growth of different concentrations of CO2 in the air. He needs to grow the plants in a growth chamber so that the CO2 concentration can be controlled. He has access to only two growth chambers, but each one will hold five plants. However, since the five plants in each chamber share whatever conditions are in that chamber besides the CO2 concentration, and in fact may also influence each other, they are not independent replicates but are pseudoreplicates. The growth chambers are the experimental units; the treatments are applied to the growth chambers, not to the plant independently.</p></li>
<li><p>Two fifth-grade math curricula are being studied. Two schools have agreed to participate in the study. One is randomly chosen to use curriculum A, the other to use curriculum B. At the end of the school year, the fifth-grade students in each school are tested and the results are used to do a statistical analysis comparing the two curricula. There is no true replication in this study; the students are pseudo-replicates. The schools are the experimental units; they, not the students, are randomly assigned to treatment. Within each school, the test results (and the learning) of the students in the experiment are not independent; they are influenced by the teacher and other school-specific factors (e.g., previous teachers and learning, socioeconomic background of the school, etc.).</p></li>
</ol>
</div>
</div>
<div id="stratified-random-sampling" class="section level2">
<h2><span class="header-section-number">6.3</span> Stratified Random Sampling</h2>
<p>The way in which was have selected sample units thus far has required us to know little about the population of interest in advance of selecting the sample.</p>
<p>This approach is ideal only if the characteristic of interest is distributed homogeneously across the population.</p>
<p>If, however, the characteristic is distributed heterogeneously, then estimates based on these designs will be imprecise relative to several alternative sampling designs.</p>
<p>For example, if we have information that we know to be associated with the heterogeneity in the population, we can use that ancillary information to guide alternative strategies for selecting samples that will yield estimates with higher precision that a simple random sample for the same amount of effort. The first of these designs is stratified random sampling.</p>
<p>A stratified random sample is one obtained by dividing the population elements into mutually exclusive, non-overlapping groups of sample units called strata, then selecting a simple random sample from within each stratum (stratum is singular for strata).</p>
<p>Stratifying involves classifying sampling units of the population into relatively homogeneous groups before (usually) selecting sample units. Strata are based on information other than the characteristic being measured that is known to or thought to vary with the characteristic of interest.</p>
<p>Because virtually all ecological systems are heterogeneous, stratifying is used commonly as a way to increase precision in ecological studies. Common strata in ecological studies include elevation, aspect, or other geographic features for studying plant communities and vegetation communities or soils for studying some animal communities. When choosing among several potential strata, seek strata that best minimize variation in the characteristic of interest within strata and that maximize variation among strata.</p>
<p>How it is implemented: * Divide the entire population into non-overlapping strata * Select a simple random sample from within each strata</p>
<p><em>L</em> = number of strata <span class="math inline">\(n_i\)</span> = number of sample units within stratum <em>i</em> <em>n</em> = number of sample units in the population</p>
<p>Estimates from stratified random samples are simply the weighted average or the sum of estimates from a series of simple random samples, each generated within a unique stratum.</p>
<p><span class="math inline">\(\bar{X} = \frac{1}{n}\sum_{i = 1}^{L}n_{i}\bar{X}_i\)</span></p>
<p><span class="math inline">\(s^2 = \frac{1}{n^2}\sum_{i = 1}^{L}n_i^2(\frac{n - n_i}{n})(\frac{s_i^2}{n_i})\)</span></p>
<div id="allocating-sampling-effort-among-strata" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Allocating Sampling Effort among Strata</h3>
<p>Using stratified random sampling requires that we decide how to divide a fixed amount of sampling effort among the different strata; that process is called allocation. When deciding where to allocate sampling effort, the question becomes how best to allocate effort among strata so that the sampling process will provide the most efficient balance of effort, cost, and estimate precision.</p>
<ul>
<li><p>Uniform Allocation</p></li>
<li><p>Allocation Proportional to Size or Variation</p></li>
</ul>
</div>
</div>
<div id="parameter-estimates---unbiased" class="section level2">
<h2><span class="header-section-number">6.4</span> Parameter Estimates - Unbiased</h2>
<ul>
<li><p>Parameters are estimated quantities that could describe any number of population characteristics (the characteristics of variables).</p></li>
<li><p>We are generally focused on the mean and variance (however estimated parameters could be the mean, median, mode, proportion, total, etc.).</p></li>
<li><p>One aspect of parameter estimation is to get a <em>point estimate</em>, an approximation of the true value of the population parameter. However this estimated does not provide information about the precision of the estimate.</p></li>
<li><p>The best estimate of <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\bar{X}\)</span>.</p></li>
<li><p>The best estimate of <span class="math inline">\(\sigma\)</span> is <span class="math inline">\(s\)</span>.</p></li>
<li><p>We strive for an unbiased estimate of population parameters: Unbiased means that the expected value is equal to the true value (accuracy)</p></li>
</ul>
</div>
<div id="parameter-estimates---efficient" class="section level2">
<h2><span class="header-section-number">6.5</span> Parameter Estimates - Efficient</h2>
<ul>
<li><p>The second property we want is to have parameters be efficient</p></li>
<li><p>This has to do with the relationship of variance to sample size and is concerned with understanding how does sampling impact the estimate - variance should be reduced when sampling is increased.</p></li>
</ul>
</div>
<div id="interval-estimation" class="section level2">
<h2><span class="header-section-number">6.6</span> Interval Estimation</h2>
<ul>
<li><p>Reporting point estimates alone is unsatisfactory - we need an estimate of the variation.</p></li>
<li><p>We want to know how close the sample statistic is to the population parameter.</p></li>
<li><p>Interval estimation is concerned making statements saying how confident or certain we are that the populaiton parameter resides in an interval of values.</p></li>
<li><p>e.g. There is a 0.95 probability that the value of <span class="math inline">\(\mu\)</span> resides between 6.2 and 9.7.</p></li>
</ul>
</div>
<div id="confidence-interval" class="section level2">
<h2><span class="header-section-number">6.7</span> Confidence Interval</h2>
<ul>
<li>Confidence intervals involve:</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Identification of associated probability</p></li>
<li><p>Specification of the interval</p></li>
</ol>
<ul>
<li><p>So how do we determine confidence intervals in practice:</p></li>
<li><p>In the first case, let’s use or general equation for a statistical model:</p></li>
</ul>
<p>Statistic = Parameter <span class="math inline">\(\pm\)</span> Error (From Kachigan, p. 138)</p>
<p>Parameter = Statistic <span class="math inline">\(\pm\)</span> Error</p>
<p><span class="math inline">\(\mu = \bar{X} \pm Error\)</span></p>
<ul>
<li>We will never know the magnitude of the true error.</li>
</ul>
<p>What we can understand is the combined effect of sampling and process error. so, we attach a probability that the error is a certain size. Now, we want to solve for the Statistic.</p>
<p><span class="math inline">\(\bar{X} = \mu \pm Error\)</span></p>
<ul>
<li><p>If we take an infinite number of samples from the population, we will get an estimate of the error term, <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>The error in sampling, from an <em>infinite</em> number of samples, is equal to the variation in the sample means, the standard distribution (sd) equals the standard error of the mean.</p></li>
<li><p>So this takes us back to our <em>Z</em>-score:</p></li>
<li><p>What is the z value in which 95% of the values are under the curve?</p></li>
<li><p>Let’s look at a <a href="http://www.z-table.com/">Z Table</a></p></li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p><span class="math inline">\(\mu = \bar{X} \pm 1.96\sigma\)</span> (95% Certainty)</p>
<p><span class="math inline">\(\mu = \bar{X} \pm ? \sigma\)</span> (99% Certainty)</p>
<ul>
<li>The above presumes that we have a very good knowledge of the population parameter <span class="math inline">\(\sigma\)</span>, which is generally not the case. We do not sample populations and infinite number of times.</li>
</ul>
</div>
<div id="confidence-intervals-of-the-mean-from-samples---using-the-t-distribution" class="section level2">
<h2><span class="header-section-number">6.8</span> Confidence Intervals of the Mean from samples - using the t-distribution</h2>
<ul>
<li><p>The determination of the sampling interval from populations with unknown <span class="math inline">\(\sigma\)</span> we will use our best estimate of <span class="math inline">\(\sigma\)</span>, which is <em>s</em>.</p></li>
<li><p>Similarly, we will use the <em>t</em> distribution to model the variability, you will see that it is flatter and has larger tails - what does this mean to our estimate of the confidence interval?</p></li>
<li><p>The <em>t</em> distribution is a one parameter distribution, <em>df</em> is the parameter and it controls the shape of the distribution. The model is platykurtic at small sample sizes.</p></li>
<li><p><em>df</em> is <em>n</em> - 1.</p></li>
<li><p>To model the confidence interval we assume that the distribution of the variables are normally distributed.</p></li>
<li><p>Allow us to determine the interval estimates of an estimated parameter, contingent on the values of estimates of parameters derived from sampling, in a probabilistic way.</p></li>
<li><p>Let’s look at a <a href="http://www.ttable.org/">t Table</a></p></li>
</ul>
<p><span class="math inline">\(\mu = \bar{X} \pm t_{df}{s_{\bar{X}}}\)</span> (95% Certainty, with <em>n</em> = 10?)</p>
<p><span class="math inline">\(\mu = \bar{X} \pm t_{df}{s_{\bar{X}}}\)</span> (99% Certainty, with <em>n</em> = 10?)</p>
<p><span class="math inline">\(\mu = \bar{X} \pm t_{df}{s_{\bar{X}}}\)</span> (99% Certainty, with <em>n</em> = 50?)</p>
</div>
</div>
<div id="hypothesis-testing-and-power" class="section level1">
<h1><span class="header-section-number">7</span> Hypothesis Testing and Power</h1>
<div id="statistical-hypothesis-testing" class="section level2">
<h2><span class="header-section-number">7.1</span> Statistical Hypothesis Testing</h2>
<p>Although thoroughly criticized, null hypothesis significance testing (NHST) remains the statistical method of choice used to provide evidence for an effect, in biological, biomedical and social sciences.</p>
<p>The approach plays into the primary method for making progress in science.</p>
<p>Observations -&gt; Models -&gt; Hypothesis -&gt; Null Hypothesis -&gt; Experiment</p>
<p>1.) Observations - make an observation of patterns in nature. These could be large in spatial or temporal scale, or small.</p>
<p>2.) Given the observation, construct a <em>model</em>. Models are a framework that provide an explanation about why the pattern(s) that you observe, exists.</p>
<p>3.) Hypotheses derive from models. What <em>must</em> be true for the model to have explanatory power (and thus be accurate)?</p>
<p>4.) Having established a prediction about what would happen under some experimental condition, it must be tested.</p>
<p>Inductive reasoning is used here: we can prove something to be true, instead, we can only disprove things. This is termed the <em>falsificationist procedure</em>.</p>
<p>5.) Perform the well-conducted experiment.</p>
<p>6.) Observe and interpret the outcome.</p>
<p>If you retain (fail to reject null Hypothesis) -&gt; Make more observations and develop alternative models.</p>
<p>If you reject the null hypothesis -&gt; Model is supported, does it support additional hypotheses that can be tested?</p>
</div>
<div id="nhst" class="section level2">
<h2><span class="header-section-number">7.2</span> NHST</h2>
<p>NHST is a method of statistical inference by which an experimental factor is tested against a hypothesis of no effect (the null hypothesis) or no relationship based on a given observation.</p>
<ul>
<li><p>The first step is to state a testable hypothesis:</p></li>
<li><p><span class="math inline">\(H_0\)</span> Null hypothesis</p></li>
<li><p><span class="math inline">\(H_A\)</span> Alternative hypothesis</p></li>
<li><p>Declare <span class="math inline">\(\alpha\)</span> level (this determines your quantile value)</p></li>
</ul>
<p>t is recommended to set a level of significance (a theoretical <em>p</em>-value) that acts as a reference point to identify significant results, that is to identify results that differ from the null-hypothesis of no effect. This is the <span class="math inline">\(\alpha\)</span> level.</p>
<p>Fisher recommended using <span class="math inline">\(\alpha =0.05\)</span> to judge whether an effect is significant or not.</p>
<p>This alpha level is roughly two standard deviations away from the mean for the normal distribution.</p>
<p>‘The value for which p=.05, or 1 in 20, is 1.96 or nearly 2; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not’).</p>
<p>How small the level of significance is, is left to researchers (domain knowledge).</p>
<ul>
<li><p>Collect Data</p></li>
<li><p>Compare the test statistic to the critical value (determined by <span class="math inline">\(\alpha\)</span>)</p></li>
<li><p>State the resulting probability (the <em>p</em>-value)</p></li>
</ul>
<p>The p-value is the <em>probability</em> of obtaining test results at least as extreme as the results actually observed.</p>
<p>The p-value is <em>not</em> an indication of the strength or magnitude of an effect.</p>
<p>A (small) p-value is <em>not</em> an indication favoring a given hypothesis.</p>
<p>The p-value is <em>not</em> the probability of the null hypothesis p(<span class="math inline">\(H_0\)</span>), of being true.</p>
<ul>
<li><p>State testable hypothesis</p></li>
<li><p>These are a set of mutually exclusive and exhaustive outcomes</p></li>
<li><p>The test statistic will support one or the other outcomes</p></li>
</ul>
<p><Br></p>
<p><span class="math inline">\(H_0: \mu = 0\)</span>, <span class="math inline">\(H_A:\mu \ne 0\)</span></p>
<p><span class="math inline">\(H_0: \mu = 3.5 cm\)</span>, <span class="math inline">\(H_A:\mu \ne 3.5 cm\)</span></p>
<p><span class="math inline">\(H_0: \mu = 10.5 kg\)</span>, <span class="math inline">\(H_A:\mu \ne 10.5 kg\)</span></p>
<p><Br></p>
</div>
<div id="example-use-z-score-to-evaluate-if-it-is-likely-that-a-given-value-of-the-distribution-is-the-mean-value" class="section level2">
<h2><span class="header-section-number">7.3</span> Example: Use <em>Z</em>-score to evaluate if it is likely that a given value of the distribution is the mean value</h2>
<ul>
<li><p>Is the mean fuel consumption of a population of buses equal to 20 mpg?</p></li>
<li><p>What is the null hypothesis?</p></li>
<li><p>We need information about the population (remember we are using <em>Z</em>-score so we know the population-level parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>).</p></li>
<li><p>Mean</p></li>
<li><p>Population standard deviation</p></li>
<li><p>Calculate <em>Z</em>-score for mean = 20 mpg</p></li>
<li><p>Determine the associated probability that the mean is 20 mpg given:</p></li>
<li><p><span class="math inline">\(\sigma\)</span> = 0.3, <span class="math inline">\(\mu\)</span> = 19.1</p></li>
</ul>
</div>
<div id="evaluate-z-score" class="section level2">
<h2><span class="header-section-number">7.4</span> Evaluate <em>Z</em>-score</h2>
<p>What is the probability that we would get this <em>Z</em>-score?</p>
<p>Hypothetical <em>Z</em>-scores</p>
<p><Br></p>
<pre class="r"><code>par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns
plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = &quot;Z&quot;, ylab = NA, type = &quot;l&quot;, yaxt = &#39;n&#39;, xaxt = &#39;n&#39;)
abline(v = 1.96)</code></pre>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<table>
<thead>
<tr class="header">
<th align="center">z</th>
<th align="center">.00</th>
<th align="center">.01</th>
<th align="center">.02</th>
<th align="center">.03</th>
<th align="center">.04</th>
<th align="center">.05</th>
<th align="center">.06</th>
<th align="center">.07</th>
<th align="center">.08</th>
<th align="center">.09</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.0</td>
<td align="center">.5000</td>
<td align="center">.5040</td>
<td align="center">.5080</td>
<td align="center">.5120</td>
<td align="center">.5160</td>
<td align="center">.5199</td>
<td align="center">.5239</td>
<td align="center">.5279</td>
<td align="center">.5319</td>
<td align="center">.5359</td>
</tr>
<tr class="even">
<td align="center">0.1</td>
<td align="center">.5398</td>
<td align="center">.5438</td>
<td align="center">.5478</td>
<td align="center">.5517</td>
<td align="center">.5557</td>
<td align="center">.5596</td>
<td align="center">.5636</td>
<td align="center">.5675</td>
<td align="center">.5714</td>
<td align="center">.5753</td>
</tr>
<tr class="odd">
<td align="center">0.2</td>
<td align="center">.5793</td>
<td align="center">.5832</td>
<td align="center">.5871</td>
<td align="center">.5910</td>
<td align="center">.5948</td>
<td align="center">.5987</td>
<td align="center">.6026</td>
<td align="center">.6064</td>
<td align="center">.6103</td>
<td align="center">.6141</td>
</tr>
<tr class="even">
<td align="center">0.3</td>
<td align="center">.6179</td>
<td align="center">.6217</td>
<td align="center">.6255</td>
<td align="center">.6293</td>
<td align="center">.6331</td>
<td align="center">.6368</td>
<td align="center">.6406</td>
<td align="center">.6443</td>
<td align="center">.6480</td>
<td align="center">.6517</td>
</tr>
<tr class="odd">
<td align="center">0.4</td>
<td align="center">.6554</td>
<td align="center">.6591</td>
<td align="center">.6628</td>
<td align="center">.6664</td>
<td align="center">.6700</td>
<td align="center">.6736</td>
<td align="center">.6772</td>
<td align="center">.6808</td>
<td align="center">.6844</td>
<td align="center">.6879</td>
</tr>
<tr class="even">
<td align="center">0.5</td>
<td align="center">.6915</td>
<td align="center">.6950</td>
<td align="center">.6985</td>
<td align="center">.7019</td>
<td align="center">.7054</td>
<td align="center">.7088</td>
<td align="center">.7123</td>
<td align="center">.7157</td>
<td align="center">.7190</td>
<td align="center">.7224</td>
</tr>
<tr class="odd">
<td align="center">0.6</td>
<td align="center">.7257</td>
<td align="center">.7291</td>
<td align="center">.7324</td>
<td align="center">.7357</td>
<td align="center">.7389</td>
<td align="center">.7422</td>
<td align="center">.7454</td>
<td align="center">.7486</td>
<td align="center">.7517</td>
<td align="center">.7549</td>
</tr>
<tr class="even">
<td align="center">0.7</td>
<td align="center">.7580</td>
<td align="center">.7611</td>
<td align="center">.7642</td>
<td align="center">.7673</td>
<td align="center">.7704</td>
<td align="center">.7734</td>
<td align="center">.7764</td>
<td align="center">.7794</td>
<td align="center">.7823</td>
<td align="center">.7852</td>
</tr>
<tr class="odd">
<td align="center">0.8</td>
<td align="center">.7881</td>
<td align="center">.7910</td>
<td align="center">.7939</td>
<td align="center">.7967</td>
<td align="center">.7995</td>
<td align="center">.8023</td>
<td align="center">.8051</td>
<td align="center">.8078</td>
<td align="center">.8106</td>
<td align="center">.8133</td>
</tr>
</tbody>
</table>
<pre class="r"><code>par(mfrow = c(4,4)) # This tells R to put 1 row, 1 columns
par(mar = rep(0,4))
par(oma = rep(0,4))

for (j in 1:16) {
  
  xp &lt;- c(seq(-4,4,by = 0.01))
  yp &lt;- dnorm(xp)
  plot(xp, yp, type = &quot;l&quot;, yaxt = &#39;n&#39;, xaxt = &#39;n&#39;,xlab =&quot;&quot;,ylab = &quot;&quot;)
  
  xp &lt;- c(seq(-4,seq(0,1.99,length.out = 16)[j],by = 0.1))
  yp &lt;- c(dnorm(xp), rep(0,c(length(xp))))
  xp &lt;- c(xp, rev(xp))

  polygon(x = xp, y = yp, col = &#39;steelblue&#39;)  
  
  legend(&quot;topleft&quot;, 
         paste0(&#39;z =&#39;,round(max(xp),2)),
         bty = &#39;n&#39;)  
    legend(&quot;topright&quot;, 
         paste0(&#39;Area =&#39;,round(pnorm(max(xp)),2)),
         bty = &#39;n&#39;)  
  abline(v = 0, lwd = 2)
}</code></pre>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
<div id="is-it-meaningful" class="section level2">
<h2><span class="header-section-number">7.5</span> Is it meaningful?</h2>
<p>Declare <span class="math inline">\(\alpha\)</span></p>
<ul>
<li>Given our <span class="math inline">\(\alpha\)</span> level, how does the resulting probability compare?</li>
<li>Remember, <span class="math inline">\(\alpha\)</span> is defined prior to statistical testing</li>
<li>Two tail and one tail test</li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
</div>
<div id="statistical-hypothesis-testing-1" class="section level2">
<h2><span class="header-section-number">7.6</span> Statistical Hypothesis Testing</h2>
<p>Evaluate if the population mean is not significantly different from some specified value.</p>
<p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu\)</span> = 0</p>
<p><span class="math inline">\(H_A: \mu \ne 0\)</span></p>
<p>Introduce the idea of a critical value (critical quantile)</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> of 0.05</li>
</ul>
<p>We have data taken from the weight change in horses given some medical treatment.</p>
<p>We are interested to know if the mean change in weight that we found +1.29 kg is significantly different from 0 kg.</p>
<ul>
<li>We calculate the <em>Z</em>-score and find that <em>Z</em> = 1.45</li>
</ul>
<p><span class="math inline">\(P(mean \ge 1.29) = P(Z \ge 1.45) = ?\)</span></p>
<p><span class="math inline">\(P(mean \le 1.29) = P(Z \le 1.45) = ?\)</span></p>
<p>Z = 1.96 is the rejection region at 2.5%</p>
<ul>
<li>This is the ‘region of rejection’</li>
</ul>
<p>Now we have a way to objectively reject or accept the null hypothesis.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p><Br></p>
<div id="one--and-two-tailed-tests" class="section level3">
<h3><span class="header-section-number">7.6.1</span> One- and Two-Tailed Tests</h3>
<p>Alternative to testing ‘is the value different.’</p>
<p>In some cases we care about the direction of the difference (is the value less than or greater than some value).</p>
<p>Use one-tailed test</p>
<ul>
<li>In general, one-tailed hypotheses about a mean are:</li>
<li><span class="math inline">\(H_0:\mu\ge\mu_0\)</span> and <span class="math inline">\(H_A:\mu&lt;\mu_0\)</span></li>
<li>In which case, H<sub>0</sub> is rejected if the test statistic is in the left-hand tail of the distribution or:</li>
<li><span class="math inline">\(H_0:\mu\le\mu_0\)</span> and <span class="math inline">\(H_A:\mu&gt;\mu_0\)</span></li>
</ul>
<p>Contrast the region of rejection for these.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p><Br></p>
</div>
</div>
<div id="type-1-and-type-2-errors" class="section level2">
<h2><span class="header-section-number">7.7</span> Type-1 and Type-2 Errors</h2>
<p>Sometimes we:</p>
<ul>
<li>Reject the null hypothesis when it is true.</li>
<li>Accept the alternative hypothesis when it is false.</li>
</ul>
<p>Type 1 error or alpha error - frequency of rejecting <span class="math inline">\(H_0\)</span> when it is true.</p>
<p>Type 1 error rate is equal to <span class="math inline">\(\alpha\)</span>.</p>
<p>Type 1 error: “rejecting the null hypothesis when it is true.” We rejected the null hypothesis but did so erroneously.</p>
<p>Type 1 error is termed ‘<span class="math inline">\(\alpha\)</span> error’ because it is equal to <span class="math inline">\(\alpha\)</span></p>
<p>Now we have some criteria to choose alpha.</p>
<p>So if your <span class="math inline">\(\alpha\)</span>, or critical value is 0.10 we have a 10% probability of rejecting the null hypothesis when we should have, in fact, accepted it.</p>
<div id="type-1-alpha-error" class="section level3">
<h3><span class="header-section-number">7.7.1</span> Type 1 (<span class="math inline">\(\alpha\)</span>) Error</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p><Br></p>
<p>Type 2 error: “accepting the null hypothesis when it is false.”</p>
<p>Type 2 error or ‘<span class="math inline">\(\beta\)</span> error’ is equal to <span class="math inline">\(\beta\)</span>.</p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>If H<sub>0</sub> is true</th>
<th>If H<sub>0</sub> is false</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>If H<sub>0</sub> is rejected</td>
<td>Type I error</td>
<td>No error</td>
</tr>
<tr class="even">
<td>If H<sub>0</sub> is not rejected</td>
<td>No error</td>
<td>Type II error</td>
</tr>
</tbody>
</table>
<p><em>Table 4.8.1:  Two Types of Errors in Hypothesis Testing</em></p>
<p><Br></p>
<p>Thought experiments:</p>
<ul>
<li>Ex. Endangered species conservation</li>
<li>Ex. Pharmaceutical testing</li>
</ul>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>If H<sub>0</sub> is true</th>
<th>If H<sub>0</sub> is false</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>If H<sub>0</sub> is rejected</td>
<td><span class="math inline">\(\alpha\)</span></td>
<td><span class="math inline">\(1-\beta\)</span> (“power”) No error</td>
</tr>
<tr class="even">
<td>If H<sub>0</sub> is not rejected</td>
<td>No error <span class="math inline">\(1-\alpha\)</span></td>
<td><span class="math inline">\(\beta\)</span></td>
</tr>
</tbody>
</table>
<p><em>Table 4.8.2:  Long-term Probabilities of Outcomes in Hypothesis Testing</em></p>
</div>
<div id="statistical-power" class="section level3">
<h3><span class="header-section-number">7.7.2</span> Statistical Power</h3>
<p>Power: the <em>probability</em> that a statistical test will reject a null hypothesis when it is false (proper rejection).</p>
<p>There are multiple ways to interpret power correctly:</p>
<ul>
<li><p>Power is the probability of rejecting the null hypothesis when, in fact, it is false.</p></li>
<li><p>Power is the probability of making a correct decision (to reject the null hypothesis) when the null hypothesis is false.</p></li>
<li><p>Power is the probability that a test of significance will pick up on an effect that is present.</p></li>
<li><p>Power is the probability that a test of significance will detect a deviation from the null hypothesis, should such a deviation exist.</p></li>
<li><p>Power is the probability of avoiding a Type II error.</p></li>
</ul>
<p>Power is 1 – <span class="math inline">\(\beta\)</span>.</p>
<p>The power of a hypothesis test is between 0 and 1;</p>
<p>If the power is close to 1, the hypothesis test is very good at detecting a false null hypothesis.</p>
<p>Beta is commonly set at 0.2, but may be set by the researchers to be smaller. Consequently, power may be as low as 0.8, but may be higher. Powers lower than 0.8, while not impossible, would typically be considered too low for most areas of research.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
</div>
<div id="leafs-power-simulation-in-r" class="section level3">
<h3><span class="header-section-number">7.7.3</span> Leaf’s power simulation in R</h3>
<p><Br></p>
</div>
<div id="what-influences-statistical-power" class="section level3">
<h3><span class="header-section-number">7.7.4</span> What Influences Statistical Power?</h3>
<p>These are listed in Zar, we can take them step by step to learn why power is influenced.</p>
<ol start="0" style="list-style-type: decimal">
<li><p>Significance level (or alpha)</p></li>
<li><p>Sample Size</p></li>
</ol>
<p>Power depends on sample size. Other things being equal, larger sample size yields higher power.</p>
<ol start="2" style="list-style-type: decimal">
<li>Variance</li>
</ol>
<p>Power also depends on variance: smaller variance yields higher power.</p>
<ol start="3" style="list-style-type: decimal">
<li>Experimental Design</li>
</ol>
<p>Power can sometimes be increased by adopting a different experimental design that has lower error variance. For example, stratified sampling can reduce error variance and hence increase power. However,</p>
<ul>
<li><p>The power calculation will depend on the experimental design.</p></li>
<li><p>The statistical analysis will depend on the experimental design.</p></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Magnitude of the effect of the variable.</li>
</ol>
</div>
</div>
</div>
<div id="parameteric-and-non-parameteric-correlation" class="section level1">
<h1><span class="header-section-number">8</span> Parameteric and Non-Parameteric Correlation</h1>
<div id="correlation" class="section level2">
<h2><span class="header-section-number">8.1</span> Correlation</h2>
<p>We often have information on two numeric characteristics for each member of a group and are interested in finding the degree of association between these characteristics.</p>
<p>For instance, an obstetrician may decide to look up the records of women who delivered in her hospital in the previous year to find out whether there is a relationship between their family incomes and the birth weights of their babies.</p>
<p>The relationship here means whether the two variables fluctuate together, i.e., does thebirth weight increase (or decrease) as the income increases.</p>
<p>Parametric approaches</p>
<ul>
<li>Pearson’s correlation coefficient</li>
</ul>
<p>Nonparametric approaches</p>
<ul>
<li>Spearman’s rho</li>
<li>Kendall’s tau</li>
</ul>
</div>
<div id="interpreting-correlations" class="section level2">
<h2><span class="header-section-number">8.2</span> Interpreting correlations</h2>
<div id="positive-relationship" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Positive Relationship</h3>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:sm&#39;:
## 
##     muscle</code></pre>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
<div id="negative-relationship" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Negative Relationship</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
</div>
<div id="no-relationship" class="section level3">
<h3><span class="header-section-number">8.2.3</span> No Relationship</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
</div>
</div>
<div id="quantifying-the-magnitude-of-correlation" class="section level2">
<h2><span class="header-section-number">8.3</span> Quantifying the magnitude of correlation</h2>
<ul>
<li><p>As one variable increases, does the other increase, decrease or not change at all (stay the same)?</p></li>
<li><p>This can be done by understood by calculating the covariance.</p></li>
<li><p>We look at how much each score deviates from their respective mean values.</p></li>
<li><p>If both variables deviate from the mean in a similar way, they are likely to be related (or ‘covary’).</p></li>
</ul>
<p>Here is the results (bivariate) of an experiment aimed at understanding the efficacy of advertising:</p>
<table>
<thead>
<tr class="header">
<th align="left">Participant number (<em>i</em>)</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">Mean</th>
<th align="center">SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Adverts Watched (<em>X</em>)</td>
<td align="center">5</td>
<td align="center">4</td>
<td align="center">4</td>
<td align="center">6</td>
<td align="center">8</td>
<td align="center">5.4</td>
<td align="center">1.67</td>
</tr>
<tr class="even">
<td align="left">Packets Bought (<em>Y</em>)</td>
<td align="center">8</td>
<td align="center">9</td>
<td align="center">10</td>
<td align="center">13</td>
<td align="center">15</td>
<td align="center">11</td>
<td align="center">2.92</td>
</tr>
</tbody>
</table>
<p>Residual error values:</p>
<p>Remember residuals are the value of the observation - expectation.</p>
<table>
<thead>
<tr class="header">
<th align="left">Participant number (<em>i</em>)</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">Mean</th>
<th align="center">SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Adverts Watched</td>
<td align="center">5</td>
<td align="center">4</td>
<td align="center">4</td>
<td align="center">6</td>
<td align="center">8</td>
<td align="center">5.4</td>
<td align="center">1.67</td>
</tr>
<tr class="even">
<td align="left">Packets Bought</td>
<td align="center">8</td>
<td align="center">9</td>
<td align="center">10</td>
<td align="center">13</td>
<td align="center">15</td>
<td align="center">11</td>
<td align="center">2.92</td>
</tr>
<tr class="odd">
<td align="left">Advertiser Residual</td>
<td align="center">-0.4</td>
<td align="center">-1.4</td>
<td align="center">-1.4</td>
<td align="center">0.6</td>
<td align="center">2.6</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Packets residual</td>
<td align="center">-3</td>
<td align="center">-2</td>
<td align="center">-1</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
</div>
<div id="covariance-and-a-re-examination-of-variance" class="section level2">
<h2><span class="header-section-number">8.4</span> Covariance and a re-examination of variance</h2>
<ul>
<li><p>Remember the variance tells us how much scores deviate from the mean for a single variable.</p></li>
<li><p>It is closely linked to the sum of squares, indeed we use the sum of squares to calculate the variance.</p></li>
<li><p>Covariance is similar - it tells is by how much scores on two variables differ from their respective means.</p></li>
</ul>
<p><span class="math inline">\(s^2=\frac{\Sigma(X_i - \bar{X})}{n-1}^2\)</span></p>
<p><span class="math inline">\(s^2=\frac{\Sigma(X_i - \bar{X})(X_i - \bar{X})}{n-1}\)</span></p>
</div>
<div id="here-we-are-examining-the-covariance-so-the-calculation-changes-a-bit." class="section level2">
<h2><span class="header-section-number">8.5</span> Here we are examining the ‘covariance’ so the calculation changes a bit.</h2>
<ul>
<li><p>Calculate the error (residual value) between the mean and each subject’s score for the first variable (<em>X</em>).</p></li>
<li><p>Calculate the error (residual value) between the mean and their score for the second variable (<em>y</em>).</p></li>
<li><p>Multiply the error values (the residual values).</p></li>
<li><p>Add these values and you get the cross product deviations.</p></li>
<li><p>The covariance is the mean (average) of the cross-product deviations:</p></li>
</ul>
<p><span class="math inline">\(cov(X,Y)=\frac{\Sigma(X_i - \bar{X})(Y_i - \bar{Y})}{n-1}\)</span></p>
<p>From our example, and plugging in the values:</p>
<p><span class="math inline">\(cov(X,Y)=\frac{(-0.4)(-3)+(-1.4)(-2)+(-1.4)(-1)+(0.6)(2)+(2.6)(4)}{4}\)</span></p>
<p><span class="math inline">\(cov(X,Y)=\frac{1.2+2.8+1.4+1.2+10.4}{4}\)</span></p>
<p><span class="math inline">\(cov(X,Y)=\frac{17}{4}\)</span></p>
<p><span class="math inline">\(cov(X,Y)=4.25\)</span></p>
</div>
<div id="limitations-of-covariance" class="section level2">
<h2><span class="header-section-number">8.6</span> Limitations of Covariance</h2>
<p>The magnitude of the covariance is dependent on the units of measurement.</p>
<ul>
<li><p>e.g. the covariance of two variables measured in miles might be 4.25, but if the same scores are converted to kilometres, the covariance is changed…</p></li>
<li><p>To address this issue we can standarize the covariance value by standardization: Divide by the standard deviations of both variables. The standardized version of covariance is known as the correlation coefficient. It is unaffected by units of measurement.</p></li>
</ul>
</div>
<div id="the-correlation-coefficient" class="section level2">
<h2><span class="header-section-number">8.7</span> The Correlation Coefficient</h2>
<p><span class="math inline">\(r=\frac{cov_{X,Y}}{s_Xs_Y}\)</span></p>
<p><span class="math inline">\(r=\frac{\Sigma(X_i - \bar{X})(Y_i - \bar{Y})}{(n-1)s_Xs_Y}\)</span></p>
<p><span class="math inline">\(r=\frac{cov_{XY}}{s_Xs_Y}\)</span></p>
<p><span class="math inline">\(r=\frac{4.25}{1.67 * 2.92}\)</span></p>
<p><span class="math inline">\(r=0.87\)</span></p>
<p>Termed Pearson-product moment correlation coefficient</p>
<p>It ranges between -1 and +1</p>
<ul>
<li>A value of zero, indicates that there is no relationship</li>
</ul>
<p>It is a testable hypothesis</p>
<p>Testing <span class="math inline">\(H_0: \rho=0\)</span> versus <span class="math inline">\(H_A: \rho\ne0\)</span></p>
<p>The standard error of the correlation coefficient is calculated as:</p>
<p><span class="math inline">\(S_r=\sqrt\frac{1-r^2}{n-2}\)</span></p>
<p>It is a testable hypothesis</p>
<p><em>r</em> = 0.870</p>
<p><em>n</em> = 12 (new data set, with more samples)</p>
<p>We will calculate the critical value:</p>
<p><span class="math inline">\(t=\frac{r}{S_r}= \frac{0.870}{0.156}= 5.58\)</span></p>
<p>t<sub>0.05(2),10</sub> =2.228</p>
<p>Testing <span class="math inline">\(H_0: \rho=0\)</span> versus <span class="math inline">\(H_A: \rho\ne0\)</span></p>
<p>Coefficient of determination, r^2</p>
<ul>
<li>By squaring the value of r you get the proportion of variance in one variable shared by the other.</li>
</ul>
<p>Square of correlation coefficient (<span class="math inline">\(r^2\)</span>), known as coefficient of determination, represents the proportion of variation in one variable that is accounted for by the variation in the other variable.</p>
<p>For example, if height and weight of a group of persons have a correlation coefficient of (<span class="math inline">\(\rho = 0.80\)</span>), one can estimate that 64% (0.80 × 0.80 = 0.64) of variation in their weights is accounted for by the variation in their heights.</p>
</div>
<div id="non-parametric-correlation" class="section level2">
<h2><span class="header-section-number">8.8</span> Non-parametric Correlation</h2>
<p>Spearman’s rho <span class="math inline">\(\rho\)</span></p>
<ul>
<li>Pearson’s correlation on the ranked data</li>
</ul>
<p>Kendall’s tau (<span class="math inline">\(\tau\)</span>)</p>
<ul>
<li>“Better” than Spearman’s for small samples</li>
</ul>
</div>
<div id="spearman-rank-correlation-coefficient" class="section level2">
<h2><span class="header-section-number">8.9</span> Spearman Rank Correlation Coefficient</h2>
<p><em>d</em> is the difference between two numbers in each pair of ranks</p>
<p><em>n</em> = number of pairs of data</p>
<p><span class="math inline">\(r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})\)</span></p>
<table>
<thead>
<tr class="header">
<th>Data 1</th>
<th>Data 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>2</td>
</tr>
<tr class="even">
<td>4</td>
<td>9</td>
</tr>
<tr class="odd">
<td>7</td>
<td>3</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th>Data 1</th>
<th>Data 2</th>
<th>Rank 1</th>
<th>Rank 2</th>
<th>d</th>
<th>d<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td>9</td>
<td>1</td>
<td>3</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th>Data 1</th>
<th>Data 2</th>
<th>Rank 1</th>
<th>Rank 2</th>
<th>d</th>
<th>d<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>4</td>
<td>9</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>4</td>
</tr>
<tr class="odd">
<td>7</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})\)</span></p>
<p><span class="math inline">\(=1-(\frac{6*6}{3(3^2 - 1)})\)</span></p>
<p>We can use this value as the calculated <em>r</em> value</p>
<p>The critical value is a two tailed value with <em>n</em></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
